{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41535fd",
   "metadata": {},
   "source": [
    "# Assignment Module 2: Pet Classification\n",
    "\n",
    "The goal of this assignment is to implement a neural network that classifies images of 37 breeds of cats and dogs from the [Oxford-IIIT-Pet dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/). The assignment is divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1476550",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The following cells contain the code to download and access the dataset you will be using in this assignment. Note that, although this dataset features each and every image from [Oxford-IIIT-Pet](https://www.robots.ox.ac.uk/~vgg/data/pets/), it uses a different train-val-test split than the original authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91101a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/CVLAB-Unibo/ipcv-assignment-2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d8fb0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c9929",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OxfordPetDataset(Dataset):\n",
    "    def __init__(self, split: str, transform=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = Path(\"ipcv-assignment-2\") / \"dataset\"\n",
    "        self.split = split\n",
    "        self.names, self.labels = self._get_names_and_labels()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n",
    "        img_path = self.root / \"images\" / f\"{self.names[idx]}.jpg\"\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "    def get_num_classes(self) -> int:\n",
    "        return max(self.labels) + 1\n",
    "\n",
    "    def _get_names_and_labels(self) -> Tuple[List[str], List[int]]:\n",
    "        names = []\n",
    "        labels = []\n",
    "\n",
    "        with open(self.root / \"annotations\" / f\"{self.split}.txt\") as f:\n",
    "            for line in f:\n",
    "                name, label = line.replace(\"\\n\", \"\").split(\" \")\n",
    "                names.append(name), \n",
    "                labels.append(int(label) - 1)\n",
    "\n",
    "        return names, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e655bd",
   "metadata": {},
   "source": [
    "## Part 1: design your own network\n",
    "\n",
    "Your goal is to implement a convolutional neural network for image classification and train it from scratch on `OxfordPetDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the test split of ~60%. You are free to achieve this however you want, except for a few rules you must follow:\n",
    "\n",
    "- Compile this notebook by displaying the results obtained by the best model you found throughout your experimentation; then show how, by removing some of its components, its performance drops. In other words, do an *ablation study* to prove that your design choices have a positive impact on the final result.\n",
    "\n",
    "- Do not instantiate an off-the-self PyTorch network. Instead, construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you cannot use e.g. `torchvision.models.alexnet`.\n",
    "\n",
    "- Show your results and ablations with plots, tables, images, etc. — the clearer, the better.\n",
    "\n",
    "Don't be too concerned with your model performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded more points than a poorly experimentally validated model with higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192fb2ce",
   "metadata": {},
   "source": [
    "## Part 2: fine-tune an existing network\n",
    "\n",
    "Your goal is to fine-tune a pretrained ResNet-18 model on `OxfordPetDataset`. Use the implementation provided by PyTorch, i.e. the opposite of part 1. Specifically, use the PyTorch ResNet-18 model pretrained on ImageNet-1K (V1). Divide your fine-tuning into two parts:\n",
    "\n",
    "2A. First, fine-tune the ResNet-18 with the same training hyperparameters you used for your best model in part 1.\n",
    "\n",
    "2B. Then, tweak the training hyperparameters in order to increase the accuracy on the test split. Justify your choices by analyzing the training plots and/or citing sources that guided you in your decisions — papers, blog posts, YouTube videos, or whatever else you may find useful. You should consider yourselves satisfied once you obtain a classification accuracy on the test split of ~90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "\n",
    "data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a51d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = OxfordPetDataset('train', transform=transform_train)\n",
    "val_dataset = OxfordPetDataset('val', transform=transform_test)\n",
    "test_dataset = OxfordPetDataset('test', transform=transform_test)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "num_classes = train_dataset.get_num_classes()\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Val samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "def create_resnet18_model(num_classes, pretrained=True):\n",
    "    \"\"\"Create ResNet-18 model with modified final layer\"\"\"\n",
    "    model = models.resnet18(pretrained=pretrained)\n",
    "    # Replace the final fully connected layer\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, model_name):\n",
    "    \"\"\"Train model and return training history\"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc.item())\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        val_epoch_loss = val_running_loss / len(val_dataset)\n",
    "        val_epoch_acc = val_running_corrects.double() / len(val_dataset)\n",
    "        \n",
    "        history['val_loss'].append(val_epoch_loss)\n",
    "        history['val_acc'].append(val_epoch_acc.item())\n",
    "        \n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        print(f'Val Loss: {val_epoch_loss:.4f} Acc: {val_epoch_acc:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            # Save checkpoint\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'history': history\n",
    "            }, f'best_{model_name}_checkpoint.pth')\n",
    "        \n",
    "        # Step scheduler\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - start_time\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_val_acc:.4f}')\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history, best_val_acc\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    test_running_corrects = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_running_loss += loss.item() * inputs.size(0)\n",
    "            test_running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_loss = test_running_loss / len(test_dataset)\n",
    "    test_acc = test_running_corrects.double() / len(test_dataset)\n",
    "    \n",
    "    return test_loss, test_acc.item(), all_preds, all_labels\n",
    "\n",
    "def plot_training_history(history, title):\n",
    "    \"\"\"Plot training and validation curves\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history['train_loss'], label='Train Loss')\n",
    "    ax1.plot(history['val_loss'], label='Val Loss')\n",
    "    ax1.set_title(f'{title} - Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history['train_acc'], label='Train Accuracy')\n",
    "    ax2.plot(history['val_acc'], label='Val Accuracy')\n",
    "    ax2.set_title(f'{title} - Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{title} - Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# PART 2A: Fine-tune ResNet-18 with same hyperparameters as Part 1\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"PART 2A: Fine-tuning with Part 1 hyperparameters\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create model\n",
    "model_2a = create_resnet18_model(num_classes, pretrained=True)\n",
    "\n",
    "# Use same hyperparameters as Part 1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_2a = optim.Adam(model_2a.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler (reduce LR after some epochs like in Part 1)\n",
    "scheduler_2a = optim.lr_scheduler.StepLR(optimizer_2a, step_size=60, gamma=0.1)\n",
    "\n",
    "# Train model\n",
    "model_2a, history_2a, best_val_acc_2a = train_model(\n",
    "    model_2a, train_loader, val_loader, criterion, optimizer_2a, \n",
    "    scheduler_2a, num_epochs=70, model_name=\"resnet18_2a\"\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss_2a, test_acc_2a, preds_2a, labels_2a = evaluate_model(model_2a, test_loader, criterion)\n",
    "print(f\"Part 2A Test Accuracy: {test_acc_2a:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plot_training_history(history_2a, \"Part 2A: ResNet-18 with Part 1 hyperparameters\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 2B: Optimized fine-tuning for better performance\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"PART 2B: Optimized fine-tuning\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create new model\n",
    "model_2b = create_resnet18_model(num_classes, pretrained=True)\n",
    "\n",
    "# Optimized hyperparameters for fine-tuning\n",
    "# Lower learning rate for pretrained features, higher for new classifier\n",
    "optimizer_2b = optim.Adam([\n",
    "    {'params': model_2b.fc.parameters(), 'lr': 0.001},  # New classifier layer\n",
    "    {'params': [param for name, param in model_2b.named_parameters() \n",
    "                if 'fc' not in name], 'lr': 0.0001}  # Pretrained features\n",
    "], weight_decay=1e-4)\n",
    "\n",
    "# More aggressive learning rate scheduling\n",
    "scheduler_2b = optim.lr_scheduler.MultiStepLR(optimizer_2b, milestones=[30, 60, 80], gamma=0.1)\n",
    "\n",
    "# Train model\n",
    "model_2b, history_2b, best_val_acc_2b = train_model(\n",
    "    model_2b, train_loader, val_loader, criterion, optimizer_2b, \n",
    "    scheduler_2b, num_epochs=100, model_name=\"resnet18_2b\"\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss_2b, test_acc_2b, preds_2b, labels_2b = evaluate_model(model_2b, test_loader, criterion)\n",
    "print(f\"Part 2B Test Accuracy: {test_acc_2b:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plot_training_history(history_2b, \"Part 2B: ResNet-18 with optimized hyperparameters\")\n",
    "\n",
    "# =============================================================================\n",
    "# COMPARISON AND ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "results_comparison = {\n",
    "    'Model': ['Part 2A (Same as Part 1)', 'Part 2B (Optimized)'],\n",
    "    'Test Accuracy': [f'{test_acc_2a:.4f}', f'{test_acc_2b:.4f}'],\n",
    "    'Best Val Accuracy': [f'{best_val_acc_2a:.4f}', f'{best_val_acc_2b:.4f}']\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame(results_comparison)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "models = ['Part 2A', 'Part 2B']\n",
    "test_accuracies = [test_acc_2a, test_acc_2b]\n",
    "colors = ['skyblue', 'lightgreen']\n",
    "\n",
    "bars = ax.bar(models, test_accuracies, color=colors)\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_title('ResNet-18 Fine-tuning Results Comparison')\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, test_accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrices for best model\n",
    "if test_acc_2b > test_acc_2a:\n",
    "    plot_confusion_matrix(labels_2b, preds_2b, \"Part 2B: Best Model\")\n",
    "    print(\"\\nDetailed classification report for Part 2B:\")\n",
    "    print(classification_report(labels_2b, preds_2b))\n",
    "else:\n",
    "    plot_confusion_matrix(labels_2a, preds_2a, \"Part 2A: Best Model\")\n",
    "    print(\"\\nDetailed classification report for Part 2A:\")\n",
    "    print(classification_report(labels_2a, preds_2a))\n",
    "\n",
    "# =============================================================================\n",
    "# HYPERPARAMETER JUSTIFICATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"HYPERPARAMETER JUSTIFICATION FOR PART 2B\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "justification = \"\"\"\n",
    "1. DIFFERENTIAL LEARNING RATES:\n",
    "   - Pretrained features: LR = 0.0001 (lower to preserve learned features)\n",
    "   - New classifier: LR = 0.001 (higher to learn task-specific mappings)\n",
    "   - Rationale: Pretrained features need fine adjustment, new layers need more learning\n",
    "\n",
    "2. WEIGHT DECAY (L2 Regularization):\n",
    "   - Added weight_decay=1e-4 to prevent overfitting\n",
    "   - Particularly important for small datasets like pet classification\n",
    "\n",
    "3. MULTI-STEP LEARNING RATE SCHEDULER:\n",
    "   - Reduces LR at epochs 30, 60, 80 by factor of 0.1\n",
    "   - More aggressive than single step at epoch 60\n",
    "   - Allows for better convergence in later epochs\n",
    "\n",
    "4. INCREASED TRAINING EPOCHS:\n",
    "   - 100 epochs vs 70 in Part 2A\n",
    "   - Fine-tuning often needs more epochs to reach optimal performance\n",
    "   - Early stopping via best validation accuracy prevents overfitting\n",
    "\n",
    "5. IMAGENET NORMALIZATION:\n",
    "   - Used ImageNet mean/std for preprocessing\n",
    "   - Essential for pretrained models to work properly\n",
    "   - Ensures input distribution matches training data\n",
    "\n",
    "Expected improvements:\n",
    "- Better generalization due to regularization\n",
    "- More stable training with differential learning rates\n",
    "- Higher final accuracy through extended training\n",
    "\"\"\"\n",
    "\n",
    "print(justification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
