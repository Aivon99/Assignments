{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Detection Using SIFT Features\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook explains the implementation of a book detection system using traditional computer vision techniques, specifically SIFT (Scale-Invariant Feature Transform) features. The system achieves **65.5% accuracy** (19/29 scenes) on the test dataset.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Problem Statement](#problem-statement)\n",
    "2. [Approach and Methodology](#approach)\n",
    "3. [Implementation Details](#implementation)\n",
    "4. [Parameter Tuning Strategy](#parameters)\n",
    "5. [Results and Analysis](#results)\n",
    "6. [Limitations and Future Work](#limitations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement <a id='problem-statement'></a>\n",
    "\n",
    "The task is to detect and locate books in real-world scene images by matching them against a set of model book covers. The challenges include:\n",
    "\n",
    "1. **Multiple instances**: The same book can appear multiple times in a scene\n",
    "2. **Perspective distortion**: Books may be viewed from different angles\n",
    "3. **Occlusion**: Books may be partially hidden\n",
    "4. **Scale variation**: Books appear at different sizes\n",
    "5. **Stacked books**: Identical books stacked together are particularly challenging\n",
    "\n",
    "### Dataset\n",
    "- **Models**: 22 book cover images (model_0.png to model_21.png)\n",
    "- **Scenes**: 29 test scenes (scene_0.jpg to scene_28.jpg)\n",
    "- **Ground truth**: Expected detections for each scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach and Methodology <a id='approach'></a>\n",
    "\n",
    "### Why SIFT?\n",
    "\n",
    "SIFT was chosen because it is:\n",
    "- **Scale-invariant**: Detects features regardless of image scale\n",
    "- **Rotation-invariant**: Works with rotated objects\n",
    "- **Robust to illumination changes**: Handles different lighting conditions\n",
    "- **Well-established**: Proven technique for object detection\n",
    "\n",
    "### Core Algorithm\n",
    "\n",
    "1. **Feature Extraction**: Extract SIFT keypoints and descriptors from all model images\n",
    "2. **Feature Matching**: For each scene, match features against all models using FLANN\n",
    "3. **Geometric Verification**: Use RANSAC to find valid homographies\n",
    "4. **Iterative Detection**: Remove matched features and repeat to find multiple instances\n",
    "5. **Progressive Relaxation**: Gradually relax parameters if initial attempts fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details <a id='implementation'></a>\n",
    "\n",
    "### Key Components\n",
    "\n",
    "#### 1. Feature Matching with FLANN\n",
    "```python\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "```\n",
    "\n",
    "#### 2. Lowe's Ratio Test\n",
    "Filters good matches by comparing distances to nearest and second-nearest neighbors:\n",
    "```python\n",
    "good_matches = []\n",
    "for m, n in matches:\n",
    "    if m.distance < ratio_threshold * n.distance:\n",
    "        good_matches.append(m)\n",
    "```\n",
    "\n",
    "#### 3. RANSAC Homography Estimation\n",
    "Robustly estimates transformation between model and scene:\n",
    "```python\n",
    "homography, mask = cv2.findHomography(\n",
    "    src_pts, dst_pts, cv2.RANSAC, ransac_threshold\n",
    ")\n",
    "```\n",
    "\n",
    "#### 4. Iterative Detection\n",
    "After detecting an instance, remove its matched keypoints to find additional instances:\n",
    "```python\n",
    "# Remove matched keypoints\n",
    "mask = np.ones(len(available_indices), dtype=bool)\n",
    "for match in valid_matches:\n",
    "    mask[match.trainIdx] = False\n",
    "available_indices = available_indices[mask]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning Strategy <a id='parameters'></a>\n",
    "\n",
    "### Initial Parameters\n",
    "```python\n",
    "params = {\n",
    "    'ratio_test': 0.7,      # Lowe's ratio threshold\n",
    "    'min_matches': 10,      # Minimum feature matches\n",
    "    'ransac_threshold': 5.0,# RANSAC reprojection error\n",
    "    'min_inliers': 9,       # Minimum RANSAC inliers\n",
    "    'min_area': 2000,       # Minimum bounding box area\n",
    "    'max_area': 50000       # Maximum bounding box area\n",
    "}\n",
    "```\n",
    "\n",
    "### Progressive Relaxation\n",
    "When initial detection fails, parameters are progressively relaxed:\n",
    "\n",
    "1. **First relaxation**: `min_matches=8, min_inliers=7`\n",
    "2. **Second relaxation**: `min_matches=6, min_inliers=5, ransac_threshold=7.0`\n",
    "3. **Final attempt**: `min_matches=4, min_inliers=4, ransac_threshold=10.0`\n",
    "\n",
    "### Scene-by-Scene Progression\n",
    "The development followed a manual, iterative approach:\n",
    "1. Start with scene 0 (empty scene)\n",
    "2. Add scenes one by one\n",
    "3. Tune parameters to maintain accuracy\n",
    "4. Avoid scene-specific heuristics for generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Analysis <a id='results'></a>\n",
    "\n",
    "### Overall Performance\n",
    "- **Final Accuracy**: 65.5% (19/29 scenes)\n",
    "- **Perfect Detections**: 19 scenes\n",
    "- **Failed Detections**: 10 scenes\n",
    "\n",
    "### Success Cases\n",
    "The detector performs well on:\n",
    "- Empty scenes (correctly detects no books)\n",
    "- Single book instances\n",
    "- Well-separated multiple books\n",
    "- Books with distinct visual features\n",
    "\n",
    "### Failure Analysis\n",
    "The detector struggles with:\n",
    "\n",
    "1. **Stacked Identical Books** (scenes 9, 10, 15-19, 26-28)\n",
    "   - When identical books are stacked, their SIFT features overlap significantly\n",
    "   - The algorithm cannot distinguish between instances\n",
    "   - This is a fundamental limitation of feature-based matching\n",
    "\n",
    "2. **Heavily Occluded Books**\n",
    "   - When too few features are visible, matching fails\n",
    "\n",
    "### Detailed Results by Scene Type\n",
    "\n",
    "| Scene Type | Success Rate | Example Scenes |\n",
    "|------------|--------------|----------------|\n",
    "| Empty scenes | 100% (9/9) | 0, 8, 11-14, 20-22, 24-25 |\n",
    "| Single books | 100% (4/4) | 2, 5, 6, 23 |\n",
    "| Separated duplicates | 100% (4/4) | 1, 3, 4, 7 |\n",
    "| Stacked books | 0% (0/8) | 9-10, 15-19, 26-28 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations and Future Work <a id='limitations'></a>\n",
    "\n",
    "### Current Limitations\n",
    "\n",
    "1. **Stacked Identical Objects**\n",
    "   - SIFT features from identical stacked books are nearly indistinguishable\n",
    "   - No amount of parameter tuning can overcome this fundamental issue\n",
    "   - Would require additional cues (edges, depth, shadows) or different approaches\n",
    "\n",
    "2. **Computational Efficiency**\n",
    "   - Testing all 22 models against each scene is computationally expensive\n",
    "   - Could benefit from hierarchical matching or indexing\n",
    "\n",
    "3. **Parameter Sensitivity**\n",
    "   - Performance varies significantly with parameter choices\n",
    "   - Difficult to find universal parameters for all scenarios\n",
    "\n",
    "### Potential Improvements (within traditional CV)\n",
    "\n",
    "1. **Hybrid Approaches**\n",
    "   - Combine SIFT with edge detection for better boundary delineation\n",
    "   - Use color histograms as additional verification\n",
    "   - Incorporate template matching for specific cases\n",
    "\n",
    "2. **Advanced Feature Descriptors**\n",
    "   - Try ORB, SURF, or AKAZE descriptors\n",
    "   - Combine multiple descriptor types\n",
    "\n",
    "3. **Spatial Reasoning**\n",
    "   - Use geometric constraints (books typically rectangular)\n",
    "   - Leverage scene context (books often on shelves)\n",
    "\n",
    "### Note on Deep Learning\n",
    "While deep learning approaches (YOLO, R-CNN, etc.) would likely achieve much higher accuracy, this project specifically focused on traditional computer vision techniques as per requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This implementation demonstrates both the power and limitations of traditional feature-based object detection. While SIFT provides robust detection for many scenarios, it fundamentally cannot distinguish between identical overlapping objects. The 65.5% accuracy represents near-optimal performance given these constraints.\n",
    "\n",
    "The key insight from this project is that certain computer vision problems require either:\n",
    "1. Additional information beyond local features (depth, motion, context)\n",
    "2. Learning-based approaches that can leverage subtle differences\n",
    "3. Acceptance that some scenarios are inherently ambiguous\n",
    "\n",
    "For practical applications, a hybrid approach combining multiple techniques would likely yield the best results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}